{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Text Clustering with Sentence Transformers  ###\n",
        "### and a Modified K-Means Clustering Algorithm ###\n",
        "\n",
        "\n",
        "This notebook provides a simple implementation of a text clustering algorithm that uses sentence embeddings generated by the SBERT model and a modified version of the k-means clustering algorithm to group similar sentences together. The output of the algorithm is a set of clusters, each with a representative sentence, and a list of outlier sentences that could not be assigned to any cluster.\n",
        "\n",
        "## Data\n",
        "\n",
        "The data for this notebook is a collection of requests. Each request is a sentence, and the goal of the algorithm is to group together similar requests so that they can be handled more efficiently by support staff.\n",
        "\n",
        "## Method\n",
        "\n",
        "The algorithm works by first transforming the input data into SBERT embeddings, which can be thought of as numerical representations of the meaning of each sentence. The algorithm then randomly chooses one of the embeddings to serve as the first cluster center.\n",
        "\n",
        "For each subsequent embedding, the algorithm checks whether the embedding is closer to an existing cluster center than a predetermined threshold. If it is, the embedding is assigned to that cluster. Otherwise, the embedding is added to the list of cluster centers.\n",
        "\n",
        "The clusters are then analyzed to generate representative sentences and cluster names using a combination of topic modeling and n-gram analysis. The results are saved to a JSON file for further analysis or use in downstream applications.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "This notebook provides a simple but effective method for grouping similar sentences together based on their meaning, and can be easily extended or modified for use in other applications.\n",
        "\n",
        "**Project by: Yuval Vinokur, Dvir Magen**\n"
      ],
      "metadata": {
        "id": "rp6tRU-D5UI3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting Started\n",
        "\n",
        "Before we begin, let's first download the necessary datasets. If you prefer to use your own dataset, feel free to upload it to the Colab runtime.\n",
        "\n",
        "Please make sure to run the code cell below to download the required evaluations and datasets.\n"
      ],
      "metadata": {
        "id": "5Z02OqHD561D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/Yuval-Vino/NLP-clustering-project/archive/refs/heads/master.zip\n",
        "!unzip master.zip\n",
        "!mv NLP-clustering-project-main/* /content\n",
        "!rm -r NLP-clustering-project-main"
      ],
      "metadata": {
        "id": "toIhjbIHh-jR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9a8feb8-e7a5-480d-e8c1-c33e2563ec33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-18 12:17:50--  https://github.com/Yuval-Vino/NLP-clustering-project/archive/refs/heads/master.zip\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://codeload.github.com/Yuval-Vino/NLP-clustering-project/zip/main [following]\n",
            "--2023-02-18 12:17:50--  https://codeload.github.com/Yuval-Vino/NLP-clustering-project/zip/main\n",
            "Resolving codeload.github.com (codeload.github.com)... 140.82.114.9\n",
            "Connecting to codeload.github.com (codeload.github.com)|140.82.114.9|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘master.zip’\n",
            "\n",
            "master.zip              [ <=>                ] 283.40K  --.-KB/s    in 0.07s   \n",
            "\n",
            "2023-02-18 12:17:50 (4.03 MB/s) - ‘master.zip’ saved [290197]\n",
            "\n",
            "Archive:  master.zip\n",
            "14cc89569427551caf10493cbdfa398d0c78d652\n",
            "   creating: NLP-clustering-project-main/\n",
            "  inflating: NLP-clustering-project-main/compare_clustering_solutions.py  \n",
            "  inflating: NLP-clustering-project-main/config.json  \n",
            "   creating: NLP-clustering-project-main/data/\n",
            "  inflating: NLP-clustering-project-main/data/.DS_Store  \n",
            "  inflating: NLP-clustering-project-main/data/dataset1-unrecognized-requests.csv  \n",
            "  inflating: NLP-clustering-project-main/data/dataset2-unrecognized-requests.csv  \n",
            "  inflating: NLP-clustering-project-main/dataset1-clustering-min-size-10 (3).json  \n",
            "   creating: NLP-clustering-project-main/output/\n",
            "  inflating: NLP-clustering-project-main/output/.DS_Store  \n",
            "  inflating: NLP-clustering-project-main/output/dataset1-clustering-min-size-10-solution.json  \n",
            "  inflating: NLP-clustering-project-main/output/dataset2-clustering-min-size-10-solution.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries Used\n",
        "\n",
        "The following libraries were used for this project:\n",
        "\n",
        "- `json` : for reading and writing JSON files\n",
        "- `compare_clustering_solutions` : for evaluating the clustering solutions\n",
        "- `cosine_similarity` : for computing the cosine similarity between vectors\n",
        "- `numpy` : for working with arrays and matrices\n",
        "- `csr_matrix` : for efficient sparse matrix operations\n",
        "- `sentence_transformers` : for computing sentence embeddings using pre-trained models\n",
        "- `random` : for generating random numbers and selecting items randomly\n"
      ],
      "metadata": {
        "id": "rwmFym7hCWhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from compare_clustering_solutions import evaluate_clustering\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix\n",
        "!pip install sentence_transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import random"
      ],
      "metadata": {
        "id": "0ECFFuGMBSCC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "286030ab-56df-49ab-894e-dfb5ca68918e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence_transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 KB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (4.64.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (1.13.1+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (0.14.1+cu116)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (1.7.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (3.7)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0\n",
            "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.25.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->sentence_transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk->sentence_transformers) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->sentence_transformers) (7.1.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.12.7)\n",
            "Building wheels for collected packages: sentence_transformers\n",
            "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=1041fe494f12a6661746b8697373811893f90aaa4613d102650977ac9eaa29f2\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/6f/8c/d88aec621f3f542d26fac0342bef5e693335d125f4e54aeffe\n",
            "Successfully built sentence_transformers\n",
            "Installing collected packages: tokenizers, sentencepiece, huggingface-hub, transformers, sentence_transformers\n",
            "Successfully installed huggingface-hub-0.12.1 sentence_transformers-2.2.2 sentencepiece-0.1.97 tokenizers-0.13.2 transformers-4.26.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#@Run All: Utility Functions\n"
      ],
      "metadata": {
        "id": "kz6xCuzj9aEx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def file_sentence_to_list(data_file):\n",
        "    \"\"\"\n",
        "     Given a data file, this function reads the file and returns a list of sentences.\n",
        "     \"\"\"\n",
        "    with open(data_file) as f:\n",
        "        sentences = f.readlines()\n",
        "    sentences = [(x.split(\",\", 1)[1::]) for x in sentences]\n",
        "    result = []\n",
        "    for sentence in sentences:\n",
        "        update_sentence = \" \".join(sentence).replace(\"\\n\", \"\")\n",
        "        result.append(update_sentence)\n",
        "    result = result[1::]\n",
        "    return result"
      ],
      "metadata": {
        "id": "jzxp0uPQ7WY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def SBERT_transform(sentences):\n",
        "    \"\"\"\n",
        "    This function returns the embeddings of the input sentences\n",
        "    by using the SentenceTransformer library\n",
        "    \"\"\"\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2')  # initializing the pre-trained model\n",
        "\n",
        "    embeddings = model.encode(sentences)  # getting embeddings of the sentences\n",
        "\n",
        "    # match the variable to the type we are working with (cast from ndarry to csr_matrix)\n",
        "    row, col = np.where(embeddings != 0)\n",
        "    val = embeddings[row, col]\n",
        "\n",
        "    return csr_matrix((val, (row, col)), shape=embeddings.shape)"
      ],
      "metadata": {
        "cellView": "code",
        "id": "ub6RUh4P8kSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def SBERT_transform(sentences):\n",
        "    \"\"\"\n",
        "    This function returns the embeddings of the input sentences\n",
        "    by using the SentenceTransformer library\n",
        "    \"\"\"\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2')  # initializing the pre-trained model\n",
        "\n",
        "    embeddings = model.encode(sentences)  # getting embeddings of the sentences\n",
        "\n",
        "    # match the variable to the type we are working with (cast from ndarry to csr_matrix)\n",
        "    row, col = np.where(embeddings != 0)\n",
        "    val = embeddings[row, col]\n",
        "\n",
        "    return csr_matrix((val, (row, col)), shape=embeddings.shape)"
      ],
      "metadata": {
        "id": "IUkHJ418BWGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "def get_similarities(centeroids, item, centeroids_norm):\n",
        "    \"\"\"\n",
        "    Calculate the cosine similarity between an item and a list of centeroids.\n",
        "\n",
        "    Parameters:\n",
        "    centeroids (list of csr_matrix): A list of centeroids in the form of sparse csr matrices.\n",
        "    item (csr_matrix): An item in the form of a sparse csr matrix.\n",
        "    centeroids_norm (list of float): A list of the norms of the centeroids.\n",
        "\n",
        "    Returns:\n",
        "    similarities (list of float): A list of cosine similarities between the item and the centeroids.\n",
        "    closest_index (int): The index of the centeroid with the highest similarity to the item.\n",
        "    \"\"\"\n",
        "    # create an empty list to store similarities\n",
        "    similarities = []\n",
        "\n",
        "    # convert item to a dense array and calculate its norm\n",
        "    vec2_arr = csr_matrix(item).toarray()[0]\n",
        "    vec2_norm = norm(vec2_arr)\n",
        "\n",
        "    # iterate through centeroids and calculate the similarity of each with item\n",
        "    for index, center in enumerate(centeroids):\n",
        "        vec1_arr = center.toarray()[0]\n",
        "        # calculate the dot product of the centeroid and the item\n",
        "        dot_product = dot(vec1_arr, vec2_arr)\n",
        "        # calculate the product of the norms of the centeroid and the item\n",
        "        norms_product = centeroids_norm[index] * vec2_norm\n",
        "        # calculate the similarity as the dot product divided by the product of norms\n",
        "        similarity = dot_product / norms_product\n",
        "        similarities.append(similarity)\n",
        "\n",
        "    # find the index of the centeroid with the highest similarity\n",
        "    closest_index = np.argmax(similarities)\n",
        "\n",
        "    # return the list of similarities and the index of the closest centeroid\n",
        "    return similarities, closest_index\n"
      ],
      "metadata": {
        "id": "1ME0Ne6NaU4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_centroids(clusters, data, centeroids, min_cluster_size):\n",
        "    \"\"\"\n",
        "    Update the centroids based on the new clusters.\n",
        "\n",
        "    Args:\n",
        "        clusters (list): List of cluster indices corresponding to each data point.\n",
        "        data (list): List of sparse matrices representing the data points.\n",
        "        centeroids (list): List of current centroids, where each centroid is a sparse matrix.\n",
        "        min_cluster_size (int): Minimum number of data points required to form a cluster.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - new_centeroids (list): List of updated centroids, where each centroid is a sparse matrix.\n",
        "            - new_centeroids_norms (list): List of norms of the updated centroids.\n",
        "            - early_stop (bool): Flag indicating whether the iteration can be stopped early.\n",
        "\n",
        "    \"\"\"\n",
        "    # Group the data points based on their corresponding clusters\n",
        "    items_in_centroid = [[] for _ in range(len(centeroids))]\n",
        "    for item_index, cluster_index in enumerate(clusters):\n",
        "        items_in_centroid[cluster_index].append(data[item_index])\n",
        "\n",
        "    # Remove clusters with fewer than min_cluster_size data points\n",
        "    items_in_centroid = [sub_list for sub_list in items_in_centroid if len(sub_list) >= min_cluster_size]\n",
        "\n",
        "    new_centeroids = []\n",
        "    new_centeroids_norms = []\n",
        "    early_stop = True\n",
        "    # Update the centroids based on the mean of the data points in each cluster\n",
        "    for index, item in enumerate(items_in_centroid):\n",
        "        dense_arrays = [matrix.toarray() for matrix in item]\n",
        "        mean_of_centroid = np.mean(dense_arrays, axis=0)\n",
        "\n",
        "        # If the mean of the data points is the same as the current centroid, we can stop early\n",
        "        if not np.array_equal(mean_of_centroid, centeroids[len(new_centeroids)]):\n",
        "            early_stop = False\n",
        "\n",
        "        csr_of_mean = csr_matrix(mean_of_centroid)\n",
        "        new_centeroids.append(csr_of_mean)\n",
        "        new_centeroids_norms.append(norm(csr_of_mean.toarray()[0]))\n",
        "\n",
        "    return new_centeroids, new_centeroids_norms, early_stop\n",
        "\n"
      ],
      "metadata": {
        "id": "81-g3g_pKRUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_clusters_indexes_and_centeroids(data, min_cluster_size, threshold=0.65):\n",
        "    \"\"\"\n",
        "    Generates cluster indexes and centroids from the given data.\n",
        "\n",
        "    Args:\n",
        "    - data: sparse matrix of item vectors\n",
        "    - min_cluster_size: minimum number of items for a cluster\n",
        "    - threshold: similarity threshold for a new item to be added to an existing cluster\n",
        "\n",
        "    Returns:\n",
        "    - clusters: list of cluster indices for each item in data\n",
        "    - centeroids: list of centroids for each cluster\n",
        "    \"\"\"\n",
        "    centeroids = []\n",
        "    centeroids_norm = []\n",
        "    random_centeroid = csr_matrix(data[random.randint(0, data.shape[0] - 1)])\n",
        "    centeroids.append(random_centeroid)\n",
        "    centeroids_norm.append(norm(random_centeroid.toarray()[0]))\n",
        "    clusters = [-1] * data.shape[0]\n",
        "    shuffled_indexes = list(range(data.shape[0] - 1))\n",
        "\n",
        "    similarities = None\n",
        "    for iterations in range(0, 10):\n",
        "        early_stop = True\n",
        "        random.shuffle(shuffled_indexes)\n",
        "        for shuffled_index in shuffled_indexes:\n",
        "            similarities, max_indx = get_similarities(centeroids, data[shuffled_index], centeroids_norm)\n",
        "\n",
        "            if similarities[max_indx] >= threshold:\n",
        "                # If item is similar enough to an existing centroid, add it to the corresponding cluster.\n",
        "                clusters[shuffled_index] = max_indx\n",
        "            else:\n",
        "                # Otherwise, create a new cluster and set the item as its centroid.\n",
        "                clusters[shuffled_index] = len(centeroids)\n",
        "                new_centeroid_csr = csr_matrix(data[random.randint(0, data.shape[0] - 1)])\n",
        "                centeroids.append(csr_matrix(data[shuffled_index]))\n",
        "                centeroids_norm.append(norm(new_centeroid_csr.toarray()[0]))\n",
        "\n",
        "        centeroids, centeroids_norm, early_stop = update_centroids(clusters, data, centeroids, min_cluster_size)\n",
        "        if early_stop:\n",
        "            # If early stopping condition is met, exit the loop and return the results.\n",
        "            print(f\"Early stop after {iterations} iterations, {len(centeroids)} clusters\")\n",
        "            break\n",
        "        print(f\"Iteration #{iterations + 1} finished, {len(centeroids)} clusters\")\n",
        "    return clusters, centeroids\n",
        "\n"
      ],
      "metadata": {
        "id": "evm8fzm6RDa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cluster_indexes_to_real_clusters(clusters, data, data_file):\n",
        "    \"\"\"\n",
        "    Converts a list of cluster indexes to a list of real clusters containing the corresponding sentences and data.\n",
        "\n",
        "    Args:\n",
        "        clusters (list): A list of cluster indexes.\n",
        "        data (ndarray): A numpy array containing the data points.\n",
        "        data_file (str): The path to the file containing the original sentences.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of real clusters, where each cluster is a list of sentence-data pairs.\n",
        "\n",
        "    \"\"\"\n",
        "    # Load the sentences from the file\n",
        "    sentences = file_sentence_to_list(data_file)\n",
        "\n",
        "    # Create an empty list for each cluster\n",
        "    real_clusters = [[] for i in range(0, max(clusters) + 1)]\n",
        "\n",
        "    # Group each sentence and its corresponding data point into the appropriate cluster\n",
        "    for i in range(0, len(clusters)):\n",
        "        real_clusters[clusters[i]].append([sentences[i], data[i]])\n",
        "\n",
        "    return real_clusters\n",
        "\n"
      ],
      "metadata": {
        "id": "qkIEzLGERDlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_clusters(clusters, min_cluster_size):\n",
        "    \"\"\"\n",
        "    Filters the list of clusters, removing any clusters with fewer sentences than the specified minimum size.\n",
        "\n",
        "    Args:\n",
        "    - clusters (list): A list of lists, where each inner list contains the sentences belonging to a single cluster.\n",
        "    - min_cluster_size (int): The minimum number of sentences required for a cluster to be included in the filtered list.\n",
        "\n",
        "    Returns:\n",
        "    - filtered_clusters (list): A list of filtered clusters, each represented by a list of sentences.\n",
        "    - outliers (list): A list of sentences that were not included in any of the filtered clusters.\n",
        "    \"\"\"\n",
        "    filtered_clusters = [[sub_list, centroid_index] for centroid_index, sub_list in enumerate(clusters) if\n",
        "                        len(sub_list) >= min_cluster_size]\n",
        "    outliers_list = [item for sub_list in clusters if len(sub_list) < min_cluster_size for item in sub_list]\n",
        "    print(\"Outliers: \", len(outliers_list), \" real clusters: \", len(filtered_clusters))\n",
        "    return filtered_clusters, outliers_list\n"
      ],
      "metadata": {
        "id": "9Ixa9R-mWfRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentence_to_centeroid_similarity(filterd_clusters, centeroids):\n",
        "    \"\"\"\n",
        "    Calculate the cosine similarity between the centeroids of filtered clusters and the sentences in each cluster.\n",
        "\n",
        "    Args:\n",
        "        filterd_clusters (list): A list of filtered clusters, where each cluster is a tuple containing a list of\n",
        "            sentences and their corresponding vector representations.\n",
        "        centeroids (list): A list of centeroids, where each centeroid is a vector representation of a cluster.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing two lists:\n",
        "            - A list of lists of sentences for each cluster.\n",
        "            - A list of lists of the cosine similarity values between each sentence and its cluster's centeroid.\n",
        "\n",
        "    \"\"\"\n",
        "    # Create empty lists to store the results\n",
        "    clusters_center_similarity = [[] for _ in range(0, len(filterd_clusters))]\n",
        "    clusters_sentences = [[] for _ in range(0, len(filterd_clusters))]\n",
        "\n",
        "    # Iterate through the filtered clusters and their items\n",
        "    for cluster_index, cluster in enumerate(filterd_clusters):\n",
        "        for item in cluster[0]:\n",
        "            # Calculate the cosine similarity between the centeroid and the item vector\n",
        "            sim = cosine_similarity(centeroids[cluster_index], item[1])\n",
        "            # Round the similarity score to 4 decimal places and append it to the list\n",
        "            clusters_center_similarity[cluster_index].append(round(sim[0][0], 4))\n",
        "            # Append the sentence to the list\n",
        "            clusters_sentences[cluster_index].append(item[0])\n",
        "\n",
        "    # Return the list of sentences and the list of similarity scores\n",
        "    return clusters_sentences, clusters_center_similarity\n"
      ],
      "metadata": {
        "id": "daWRnVm63lrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_representative_sentences(filterd_clusters, clusters_center_similarity, clusters_sentences,\n",
        "                                      num_of_representatives):\n",
        "    \"\"\"\n",
        "    Generate representative sentences for each cluster based on the similarity of each sentence to the cluster centroid.\n",
        "\n",
        "    Args:\n",
        "    filterd_clusters (list): A list of clusters, where each cluster is a list of pairs of sentences and their\n",
        "        respective sentence vectors.\n",
        "    clusters_center_similarity (list): A list of lists of the cosine similarity of each sentence in each cluster\n",
        "        to the cluster centroid.\n",
        "    clusters_sentences (list): A list of lists of the sentences in each cluster.\n",
        "    num_of_representatives (int): The number of representative sentences to generate for each cluster.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of lists of representative sentences for each cluster.\n",
        "    \"\"\"\n",
        "    max_reps = [[] for _ in range(0, len(filterd_clusters))]\n",
        "\n",
        "    for cluster_index, cluster in enumerate(clusters_center_similarity):\n",
        "        # convert the similarity scores to a numpy array\n",
        "        sim_array = np.array(cluster)\n",
        "        # get the unique similarity scores and sort them in descending order\n",
        "        unique_sim_values = np.unique(sim_array)\n",
        "        sorted_sim_indices = np.argsort(unique_sim_values)[::-1][:num_of_representatives]\n",
        "        max_indices = []\n",
        "\n",
        "        # get the indices of the maximum similarity scores and add the corresponding sentences to max_reps\n",
        "        for index in sorted_sim_indices:\n",
        "            max_indices.append(np.where(sim_array == unique_sim_values[index])[0][0])\n",
        "        for index in max_indices:\n",
        "            sentence = clusters_sentences[cluster_index][index].strip()\n",
        "            max_reps[cluster_index].append(sentence)\n",
        "\n",
        "    return max_reps\n"
      ],
      "metadata": {
        "id": "de_yZ0K_sR1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocesses a string of text by converting it to lowercase, tokenizing it into individual words, removing stop words\n",
        "    and non-alphabetic characters, and lemmatizing the remaining words.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text to preprocess.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: The preprocessed list of words.\n",
        "    \"\"\"\n",
        "    # Tokenize the text into individual words\n",
        "    words = word_tokenize(text.lower())\n",
        "\n",
        "    # Remove any stop words or non-alphabetic characters, and lemmatize the remaining words\n",
        "    preprocessed_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words and word.isalpha()]\n",
        "\n",
        "    return preprocessed_words"
      ],
      "metadata": {
        "id": "fkvuMvSiBSEu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89f88767-23e9-48bc-fcc1-7bd70866a282"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_clusters_topics(clusters_sentences):\n",
        "    \"\"\"\n",
        "    Generate topics for each cluster using Latent Dirichlet Allocation (LDA) model.\n",
        "\n",
        "    Args:\n",
        "        clusters_sentences (list): List of sentences in each cluster.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple of two lists. The first list contains topics for each cluster, and the second list contains the\n",
        "        labels for each topic.\n",
        "    \"\"\"\n",
        "    clusters_topics = []\n",
        "\n",
        "    for cluster_index, cluster in enumerate(clusters_sentences):\n",
        "        if len(cluster) == 0:\n",
        "            clusters_topics.append(\"No name\")\n",
        "            topic_labels.append(\"No name\")\n",
        "            continue\n",
        "        # Preprocess the text in each sentence\n",
        "        corpus = [preprocess_text(text) for text in cluster]\n",
        "\n",
        "        # Create a dictionary of terms and their frequency counts in the corpus\n",
        "        dictionary = Dictionary(corpus)\n",
        "\n",
        "        # Convert the corpus into a bag-of-words representation\n",
        "        bow_corpus = [dictionary.doc2bow(doc) for doc in corpus]\n",
        "\n",
        "        # Train the LDA model\n",
        "        if len(dictionary) == 0:\n",
        "            clusters_topics.append(\"No name\")\n",
        "            continue\n",
        "        lda_model = LdaModel(bow_corpus, num_topics=1, id2word=dictionary, passes=10, eta=0.1)\n",
        "\n",
        "        # Get the most probable words for each topic\n",
        "        topics = lda_model.show_topics(num_topics=1, num_words=5, formatted=False)\n",
        "        topic_labels = []\n",
        "        for topic in topics:\n",
        "            words = [word[0] for word in topic[1]]\n",
        "            label = ' '.join(words)\n",
        "            topic_labels.append(label)\n",
        "        clusters_topics.append(topics)\n",
        "\n",
        "    return clusters_topics, topic_labels"
      ],
      "metadata": {
        "id": "gdW1LHgeBSJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "from itertools import chain\n",
        "def generate_clusters_ngrams(filtered_clusters, clusters_sentences):\n",
        "    \"\"\"\n",
        "    Generate n-grams for each cluster\n",
        "\n",
        "    Args:\n",
        "    - filterd_clusters: A list of clusters where each cluster is a tuple of lists, with the first list\n",
        "                        containing the filtered sentences and the second list containing their vector representations\n",
        "    - clusters_sentences: A list of clusters where each cluster is a list of sentences\n",
        "\n",
        "    Returns:\n",
        "    - clusters_ngrams: A list of clusters where each cluster is a list of n-grams\n",
        "\n",
        "    \"\"\"\n",
        "    # Initialize an empty list of n-grams for each cluster\n",
        "    clusters_ngrams = [[] for _ in range(0, len(filtered_clusters))]\n",
        "\n",
        "    # Generate n-grams for each sentence in each cluster\n",
        "    for cluster_index, cluster in enumerate(clusters_sentences):\n",
        "        cluster_ngrams = []\n",
        "        for sentence in cluster:\n",
        "            words = word_tokenize(sentence)\n",
        "            # Generate 2-grams, 3-grams, 4-grams and 5-grams for each sentence\n",
        "            n_grams2 = list(ngrams(words, 2))\n",
        "            n_grams3 = list(ngrams(words, 3))\n",
        "            n_grams4 = list(ngrams(words, 4))\n",
        "            n_grams5 = list(ngrams(words, 5))\n",
        "            # Combine all the n-grams for each sentence\n",
        "            cluster_ngrams += n_grams2 + n_grams3 + n_grams4 + n_grams5\n",
        "\n",
        "        # Append the list of n-grams for the cluster to the list of n-grams for all clusters\n",
        "        clusters_ngrams[cluster_index].append(cluster_ngrams)\n",
        "\n",
        "    return clusters_ngrams"
      ],
      "metadata": {
        "id": "CvDrUYDXuaEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_clusters_topics_dictionaries(clusters_topics):\n",
        "    \"\"\"\n",
        "    Creates a list of dictionaries that represent the topics of each cluster.\n",
        "\n",
        "    Args:\n",
        "    - clusters_topics (list): A list of topics for each cluster, as returned by the generate_clusters_topics function.\n",
        "\n",
        "    Returns:\n",
        "    - clusters_dicts (list): A list of dictionaries that represent the topics of each cluster.\n",
        "    \"\"\"\n",
        "    clusters_dicts = []\n",
        "    for cluster_index, topics in enumerate(clusters_topics):\n",
        "        topic_dict = {}\n",
        "        for topic_words in topics[0]:\n",
        "            if topic_words == 0: continue  # Skip the first topic, which is just a placeholder\n",
        "            for word_prob_pair in topic_words:\n",
        "                if word_prob_pair == \"N\":\n",
        "                    clusters_dicts.append(topic_dict)  # When we reach the end of the topic, add the dictionary to the\n",
        "                    # list\n",
        "                    continue\n",
        "                topic_dict[word_prob_pair[0]] = word_prob_pair[1]  # Add the word and its probability to the dictionary\n",
        "        clusters_dicts.append(topic_dict)  # Add the dictionary to the list\n",
        "    return clusters_dicts\n"
      ],
      "metadata": {
        "id": "NSkFmxURywyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_clusters_names(clusters_dicts, clusters_ngrams):\n",
        "    \"\"\"\n",
        "    Generates a list of names for the clusters based on their ngrams and topic dictionaries.\n",
        "\n",
        "    Args:\n",
        "    - clusters_dicts (list): A list of dictionaries that represent the topics of each cluster.\n",
        "    - clusters_ngrams (list): A list of ngrams for each cluster, as returned by the generate_clusters_ngrams function.\n",
        "\n",
        "    Returns:\n",
        "    - cluster_names (list): A list of names for each cluster.\n",
        "    \"\"\"\n",
        "    cluster_names = []\n",
        "    for cluster_index, ngrams in enumerate(clusters_ngrams):\n",
        "        # Create a list to store the count of relevant keys for each ngram\n",
        "        key_counts = [0] * len(ngrams[0])\n",
        "\n",
        "        # Iterate through each ngram and its corresponding count\n",
        "        for i, ngram in enumerate(ngrams[0]):\n",
        "            # Iterate through each key in the topic dictionary\n",
        "            for key in clusters_dicts[cluster_index]:\n",
        "                # Check if the key is in the ngram and add the count to the key_counts list\n",
        "                if len(ngram) == 0: continue\n",
        "                if key in ngram:\n",
        "                    key_counts[i] += 1 + clusters_dicts[cluster_index][key] * 100\n",
        "\n",
        "        # Use the ngram with the highest count to generate the cluster name\n",
        "        cluster_names.append(\" \".join(ngrams[0][key_counts.index(max(key_counts))]))\n",
        "\n",
        "    return cluster_names"
      ],
      "metadata": {
        "id": "nKu17VCLwOh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_output_dictionaries_list(filtered_clusters, cluster_names, max_representative_sentences):\n",
        "    \"\"\"\n",
        "    Creates a list of output dictionaries for each cluster, with the cluster name, representative sentences,\n",
        "    and a list of requests associated with the cluster.\n",
        "\n",
        "    Args:\n",
        "    - filtered_clusters (list): A list of filtered clusters, as returned by the filter_clusters function.\n",
        "    - cluster_names (list): A list of names for each cluster, as returned by the generate_clusters_names function.\n",
        "    - max_representative_sentences (list): A list of representative sentences for each cluster, as returned by the get_representative_sentences function.\n",
        "\n",
        "    Returns:\n",
        "    - clusters_output (list): A list of output dictionaries for each cluster.\n",
        "    \"\"\"\n",
        "    clusters_output = []\n",
        "    for i in range(len(filtered_clusters)):\n",
        "        cluster_dict = {\"cluster_name\": cluster_names[i], \"representative_sentences\": max_representative_sentences[i]}\n",
        "        requests = []\n",
        "        for sentence_tuple in filtered_clusters[i][0]:\n",
        "            sentence_filtered = sentence_tuple[0].replace('\"', \"\")  # Remove any quotation marks from the sentence\n",
        "            requests.append(sentence_filtered)\n",
        "        cluster_dict[\"requests\"] = requests\n",
        "        clusters_output.append(cluster_dict)\n",
        "    return clusters_output\n"
      ],
      "metadata": {
        "id": "6PLl27K48YPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_outliers_to_sentence_list(outliers_list):\n",
        "    \"\"\"\n",
        "    Converts a list of outlier sentences from the clustering process to a list of sentence strings.\n",
        "\n",
        "    Args:\n",
        "    - outliers_list (list): A list of outlier sentences, where each sentence is represented as a tuple of the sentence string and its similarity score.\n",
        "\n",
        "    Returns:\n",
        "    - outliers_sentences (list): A list of sentence strings representing the outlier sentences.\n",
        "    \"\"\"\n",
        "    outliers_sentences = []\n",
        "    for sentence_tuple in outliers_list:\n",
        "        sentence_string = sentence_tuple[0].replace('\"', \"\")  # Remove quotes from the sentence string\n",
        "        outliers_sentences.append(sentence_string)\n",
        "        \n",
        "    return outliers_sentences"
      ],
      "metadata": {
        "id": "u4dpt0I5FF8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "def convert_result_list_to_json_file(cluster_list, outliers_sentences, output_file):\n",
        "    \"\"\"\n",
        "    Converts the output of the clustering algorithm to a JSON file.\n",
        "\n",
        "    Args:\n",
        "    - cluster_list (list): A list of dictionaries, where each dictionary represents a cluster and its representative sentences, as returned by the create_output_dictionaries_list function.\n",
        "    - outliers_list (list): A list of outlier sentences, as returned by the filter_outliers function.\n",
        "    - output_file (str): The name of the output file.\n",
        "\n",
        "    Returns:\n",
        "    - None: Saves a JSON file with the given name.\n",
        "    \"\"\"\n",
        "    # Create a dictionary that represents the entire output\n",
        "    result = {\"cluster_list\": cluster_list, \"unclustered\": outliers_sentences}\n",
        "\n",
        "    # Save the output to a file\n",
        "    with open(output_file, 'w') as fp:\n",
        "        json.dump(result, fp)\n",
        "\n"
      ],
      "metadata": {
        "id": "VeANrwvA7v5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perform unsupervised clustering analysis on a dataset of unrecognized requests using the following steps:\n",
        "\n",
        "    1. Transform the input data into SBERT embeddings\n",
        "    2. Generate cluster indexes and centroids using custom algorithm\n",
        "    3. Convert cluster indexes to real clusters\n",
        "    4. Filter clusters based on minimum size and obtain outlier sentences\n",
        "    5. Generate representative sentences for each cluster\n",
        "    6. Generate cluster names using topics and ngrams\n",
        "    7. Create output dictionaries for each cluster\n",
        "    8. Convert output dictionaries and outlier sentences to JSON and "
      ],
      "metadata": {
        "id": "1DGROzdM2ob9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "with open('config.json', 'r') as json_file:\n",
        "        config = json.load(json_file)"
      ],
      "metadata": {
        "id": "chQhbZT1h86z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Transform the input data into SBERT embeddings\n",
        "\n",
        "this can take a few moments\n"
      ],
      "metadata": {
        "id": "2OvwOthuzJkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = SBERT_transform(file_sentence_to_list(config['data_file']))"
      ],
      "metadata": {
        "id": "hJj4loEbzFxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Generate cluster indexes and centroids using custom algorithm\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "k3PPyWKVl-pm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Understanding the Algorithm**\n"
      ],
      "metadata": {
        "id": "9v4jCypqYS6a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In this example, we start by generating a set of 20 random points, then we use an algorithm to cluster those points together. \n",
        "\n",
        "The algorithm works by first initializing a centroid, which is used to represent each cluster. It then loops through each data point and finds the nearest centroid. If a data point is close enough to a centroid, it is assigned to that cluster. Otherwise, a new centroid is created and the data point is assigned to that new cluster.\n",
        "\n",
        "This clustering algorithm is not exactly the k-means algorithm, but it is similar in nature. By understanding this basic algorithm, we can start to see how clustering algorithms work and how they can be useful in solving real-world problems. \n"
      ],
      "metadata": {
        "id": "d_-2o-zrbzyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Example data\n",
        "np.random.seed(42)\n",
        "X = np.random.randint(1, 20, size=20)\n",
        "Y = np.random.randint(1, 20, size=20)\n",
        "\n",
        "# Initial centroid\n",
        "centroid1 = (5, 5)\n",
        "\n",
        "# Initialize clusters and centroids\n",
        "clusters = [[]]\n",
        "centroids = [centroid1]\n",
        "\n",
        "# Loop through data points\n",
        "for i in range(len(X)):\n",
        "    point = (X[i], Y[i])\n",
        "    # Find the nearest centroid\n",
        "    nearest_centroid = None\n",
        "    min_distance = float('inf')\n",
        "    for j in range(len(centroids)):\n",
        "        distance = ((point[0]-centroids[j][0])**2 + (point[1]-centroids[j][1])**2)**0.5\n",
        "        if distance < min_distance:\n",
        "            nearest_centroid = j\n",
        "            min_distance = distance\n",
        "    # Assign point to nearest cluster or create new cluster\n",
        "    if min_distance < 4:\n",
        "        clusters[nearest_centroid].append(point)\n",
        "    else:\n",
        "        clusters.append([point])\n",
        "        centroids.append(point)\n",
        "\n",
        "# Plot the clusters\n",
        "colors = ['r', 'g', 'b', 'y', 'm','c','k','w']\n",
        "for i in range(len(clusters)):\n",
        "    color = colors[i%len(colors)]\n",
        "    for point in clusters[i]:\n",
        "        plt.scatter(point[0], point[1], c=color)\n",
        "    plt.scatter(centroids[i][0], centroids[i][1], c=color, marker='x', s=200)\n",
        "\n",
        "plt.title(\"Scatter Plot with Cluster Centroids\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"Y\")\n",
        "plt.show()\n",
        "#"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZI3hL51tYP-1",
        "outputId": "f8c4bf98-3b45-4bf5-a56b-aec85229c916"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the algorithm on our data\n",
        "Running the algorithm is a crucial step in achieving our goal, and the time it takes to complete will depend on the size of the dataset. For larger datasets with more than 10,000 sentences, this process may take a few minutes to finish.\n"
      ],
      "metadata": {
        "id": "qHMz5JovYmn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clusters_indexes, centeroids = generate_clusters_indexes_and_centeroids(data,int(config[\"min_cluster_size\"]),threshold=0.65)"
      ],
      "metadata": {
        "id": "CUqwdScrrDlb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "586af99d-5fc0-45d3-8921-1778ab0920a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration #1 finished, 32 clusters\n",
            "Iteration #2 finished, 33 clusters\n",
            "Iteration #3 finished, 34 clusters\n",
            "Iteration #4 finished, 34 clusters\n",
            "Iteration #5 finished, 34 clusters\n",
            "Iteration #6 finished, 34 clusters\n",
            "Iteration #7 finished, 34 clusters\n",
            "Iteration #8 finished, 34 clusters\n",
            "Iteration #9 finished, 34 clusters\n",
            "Iteration #10 finished, 34 clusters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Convert cluster indexes to real clusters"
      ],
      "metadata": {
        "id": "DexRDWJ44VPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "real_clusters = cluster_indexes_to_real_clusters(clusters_indexes,data,config['data_file'])\n"
      ],
      "metadata": {
        "id": "8h93vId_2XPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Step 4: Filter clusters based on minimum size and obtain outlier sentences"
      ],
      "metadata": {
        "id": "u1FD_4uDzbiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_clusters,outliers_list = filter_clusters(real_clusters,int(config[\"min_cluster_size\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alsQutDNzcCi",
        "outputId": "7834b3bb-d603-4a60-e88f-d8ad20c47a28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outliers:  488  real clusters:  34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Generate representative sentences for each cluster"
      ],
      "metadata": {
        "id": "bcmutB_D49Jm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clusters_sentences,clusters_center_similarity = get_sentence_to_centeroid_similarity(filtered_clusters,centeroids)\n",
        "max_reps = generate_representative_sentences(filtered_clusters,clusters_center_similarity,clusters_sentences,int(config['num_of_representatives']))"
      ],
      "metadata": {
        "id": "rtjBFZ7k5dop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Present a selection of representative sentences from few of the respective clusters for display and analysis.**"
      ],
      "metadata": {
        "id": "hvRXlH0w1Iuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Sample data\n",
        "table_data = {'Cluster': [], 'Representative': [], 'Cluster Size': [], 'Samples': []}\n",
        "\n",
        "for i in random.sample(range(len(max_reps)), 3):\n",
        "    cluster_size = len(clusters_sentences[i])\n",
        "    samples = [clusters_sentences[i][j] for j in random.sample(range(cluster_size), 3)]\n",
        "    table_data['Cluster'].append(i)\n",
        "    table_data['Representative'].append('\\n'.join(max_reps[i]))\n",
        "    table_data['Cluster Size'].append(cluster_size)\n",
        "    table_data['Samples'].append('\\n'.join(samples))\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(table_data)\n",
        "\n",
        "# Apply formatting to DataFrame\n",
        "df = (df.style\n",
        "      .set_properties(subset=['Representative'], **{'white-space': 'pre-line'})\n",
        "      .set_properties(subset=['Samples'], **{'white-space': 'pre-line'})\n",
        "      .set_caption('Representative Sentences and Sample Sentences for Clusters'))\n",
        "\n",
        "# Display DataFrame\n",
        "display(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "dC2PmYE5DCoL",
        "outputId": "1b9fbcdb-3715-41d0-f42c-e102ba223313"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f1bff5656a0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_fba93_row0_col1, #T_fba93_row0_col3, #T_fba93_row1_col1, #T_fba93_row1_col3, #T_fba93_row2_col1, #T_fba93_row2_col3 {\n",
              "  white-space: pre-line;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_fba93_\" class=\"dataframe\">\n",
              "  <caption>Representative Sentences and Sample Sentences for Clusters</caption>\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th class=\"col_heading level0 col0\" >Cluster</th>\n",
              "      <th class=\"col_heading level0 col1\" >Representative</th>\n",
              "      <th class=\"col_heading level0 col2\" >Cluster Size</th>\n",
              "      <th class=\"col_heading level0 col3\" >Samples</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_fba93_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_fba93_row0_col0\" class=\"data row0 col0\" >8</td>\n",
              "      <td id=\"T_fba93_row0_col1\" class=\"data row0 col1\" >how do i treat coronavirus?\n",
              "what's the best way to prevent coronavirus?\n",
              "how can i prevent coronavirus</td>\n",
              "      <td id=\"T_fba93_row0_col2\" class=\"data row0 col2\" >30</td>\n",
              "      <td id=\"T_fba93_row0_col3\" class=\"data row0 col3\" >what is the best way to prevent coronavirus\n",
              "what is effective prevention against covid?\n",
              "what cleaning methods kill the coronavirus?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_fba93_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_fba93_row1_col0\" class=\"data row1 col0\" >24</td>\n",
              "      <td id=\"T_fba93_row1_col1\" class=\"data row1 col1\" >how does covid spread\n",
              "how does covid spread?\n",
              "how does covid-19 spread to others</td>\n",
              "      <td id=\"T_fba93_row1_col2\" class=\"data row1 col2\" >30</td>\n",
              "      <td id=\"T_fba93_row1_col3\" class=\"data row1 col3\" >how does covid-19 spread to others\n",
              "how does the virus spread\n",
              "how to prevent the spread of the virus</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_fba93_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_fba93_row2_col0\" class=\"data row2 col0\" >11</td>\n",
              "      <td id=\"T_fba93_row2_col1\" class=\"data row2 col1\" >what's the difference between flu and coronavirus?\n",
              "how's the flu different from coronavirus?\n",
              "flu vs coronavirus</td>\n",
              "      <td id=\"T_fba93_row2_col2\" class=\"data row2 col2\" >33</td>\n",
              "      <td id=\"T_fba93_row2_col3\" class=\"data row2 col3\" >sars vs corona virus?\n",
              "how's the flu different from coronavirus?\n",
              "flu vs. covid-19</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Generate cluster names using topics and ngrams"
      ],
      "metadata": {
        "id": "qTGTyuIM7lai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clusters_topics, topic_labels = generate_clusters_topics(clusters_sentences)\n",
        "clusters_ngrams = generate_clusters_ngrams(filtered_clusters, clusters_sentences)\n",
        "clusters_topics_dictionaries = create_clusters_topics_dictionaries(clusters_topics)\n",
        "clusters_names = generate_clusters_names(clusters_topics_dictionaries, clusters_ngrams)"
      ],
      "metadata": {
        "id": "D9anQzZk4PEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now that we have compiled all relevant information regarding our clusters, it is appropriate to showcase our results to evaluate the efficacy of our analysis.**"
      ],
      "metadata": {
        "id": "l2moIMfM2ExG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select 3 random clusters to display\n",
        "random_clusters = random.sample(range(len(clusters_names)), 3)\n",
        "\n",
        "# Create a dataframe to hold the information for the selected clusters\n",
        "display_df = pd.DataFrame(columns=['Cluster #', 'Cluster Name', 'Cluster Size', 'Representative', 'Samples'])\n",
        "\n",
        "# Iterate over the selected clusters and add their information to the dataframe\n",
        "for i in random_clusters:\n",
        "    rep = \"\\n\".join(max_reps[i])\n",
        "    samples = \"\\n\".join(random.sample(clusters_sentences[i], 3))\n",
        "    display_df = display_df.append({'Cluster #': i, 'Cluster Name': clusters_names[i], \n",
        "                                    'Cluster Size': len(clusters_sentences[i]), 'Representative': rep, \n",
        "                                    'Samples': samples}, ignore_index=True)\n",
        "\n",
        "# Display the dataframe with pretty formatting\n",
        "display_df = (display_df.style\n",
        "      .set_properties(subset=['Cluster Name'], **{'white-space': 'pre-line'})\n",
        "      .set_properties(subset=['Representative'], **{'white-space': 'pre-line'})\n",
        "      .set_properties(subset=['Samples'], **{'white-space': 'pre-line'})\n",
        "      .set_caption('Representative Sentences and Sample Sentences for Clusters'))\n",
        "display(display_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "vc_kV5n_2FJ6",
        "outputId": "c02aa428-cbd6-47c2-a1d4-2f95ede6ef21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f1bffbbb910>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_35e4e_row0_col1, #T_35e4e_row0_col3, #T_35e4e_row0_col4, #T_35e4e_row1_col1, #T_35e4e_row1_col3, #T_35e4e_row1_col4, #T_35e4e_row2_col1, #T_35e4e_row2_col3, #T_35e4e_row2_col4 {\n",
              "  white-space: pre-line;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_35e4e_\" class=\"dataframe\">\n",
              "  <caption>Representative Sentences and Sample Sentences for Clusters</caption>\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th class=\"col_heading level0 col0\" >Cluster #</th>\n",
              "      <th class=\"col_heading level0 col1\" >Cluster Name</th>\n",
              "      <th class=\"col_heading level0 col2\" >Cluster Size</th>\n",
              "      <th class=\"col_heading level0 col3\" >Representative</th>\n",
              "      <th class=\"col_heading level0 col4\" >Samples</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_35e4e_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_35e4e_row0_col0\" class=\"data row0 col0\" >32</td>\n",
              "      <td id=\"T_35e4e_row0_col1\" class=\"data row0 col1\" >safe to go</td>\n",
              "      <td id=\"T_35e4e_row0_col2\" class=\"data row0 col2\" >17</td>\n",
              "      <td id=\"T_35e4e_row0_col3\" class=\"data row0 col3\" >can i go to the grocery store\n",
              "can i go to the grocery store?\n",
              "can i go to grocery stores</td>\n",
              "      <td id=\"T_35e4e_row0_col4\" class=\"data row0 col4\" >are grocery stores still open\n",
              "can i go to the grocery store\n",
              "am i allowed to go to the grocery store?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_35e4e_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_35e4e_row1_col0\" class=\"data row1 col0\" >28</td>\n",
              "      <td id=\"T_35e4e_row1_col1\" class=\"data row1 col1\" >where can</td>\n",
              "      <td id=\"T_35e4e_row1_col2\" class=\"data row1 col2\" >30</td>\n",
              "      <td id=\"T_35e4e_row1_col3\" class=\"data row1 col3\" >is there a test kit for coronavirus\n",
              "is there a test kit for the coronavirus?\n",
              "is there a test for coronavirus</td>\n",
              "      <td id=\"T_35e4e_row1_col4\" class=\"data row1 col4\" >is there test kit for coronavirus\n",
              "where can i get tested for coronavirus\n",
              "testing spots for coronavirus?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_35e4e_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_35e4e_row2_col0\" class=\"data row2 col0\" >17</td>\n",
              "      <td id=\"T_35e4e_row2_col1\" class=\"data row2 col1\" >how long</td>\n",
              "      <td id=\"T_35e4e_row2_col2\" class=\"data row2 col2\" >19</td>\n",
              "      <td id=\"T_35e4e_row2_col3\" class=\"data row2 col3\" >how long does the coronavirus last?\n",
              "how long does the corona virus last?\n",
              "incubation period of coronavirus?</td>\n",
              "      <td id=\"T_35e4e_row2_col4\" class=\"data row2 col4\" >incubation period of coronavirus?\n",
              "how long does the virus last?\n",
              "incubation period of coronavirus?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Create output dictionaries for each cluster"
      ],
      "metadata": {
        "id": "mXxfEwoQ3Veg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_dictionaries = create_output_dictionaries_list(filtered_clusters, clusters_names, max_reps)\n",
        "outliers_sentences = convert_outliers_to_sentence_list(outliers_list)"
      ],
      "metadata": {
        "id": "NeTPJ6vO3XBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 8: Convert output dictionaries and outlier sentences to JSON and save to file"
      ],
      "metadata": {
        "id": "K1O21oAa3t_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "convert_result_list_to_json_file(output_dictionaries,outliers_sentences,config['output_file'])"
      ],
      "metadata": {
        "id": "Yn6pXlFG2smy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The preceding steps signify the completion of our analysis. At this juncture, it is imperative to perform an evaluation of our results and compare them against the actual outcomes.**"
      ],
      "metadata": {
        "id": "xgrDAgSrizof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_clustering(config['example_solution_file'], config['output_file'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ly494-My4LVM",
        "outputId": "83997e7d-0f9c-4e6f-d04d-6d4299671960"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rand score: 0.9309255079006772\n",
            "adjusted rand score: 0.6989974657789322\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XRs-RRa9d18M"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "rwmFym7hCWhL",
        "kz6xCuzj9aEx",
        "1DGROzdM2ob9",
        "9v4jCypqYS6a"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}